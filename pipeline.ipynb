{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_RELEVANT_DF_FILE = 'output/relevant_df.pkl'\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "TIMESTEPS = 200\n",
    "VOCAB_SIZE = 9896 # TODO: Find a way to calculate this dynamically and pass to the model\n",
    "BATCH_SIZE = 1 # NOTE: 4 or 1\n",
    "EPOCHS = 1\n",
    "OUTPUT_SIZE = 32\n",
    "COMPILE_PARAMS = dict(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "relevant_df = pd.read_pickle(INPUT_RELEVANT_DF_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# NOTE: If need to reimplement, https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/preprocessing/label.py#L163\n",
    "    \n",
    "y = relevant_df.pop('Reason').values\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "y = lb.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    relevant_df.pop('ReadCodeText').values,\n",
    "    y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import TransformerMixin\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "class ReadCodesToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_on_texts(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return np.array(self.texts_to_sequences(X))\n",
    "    \n",
    "readcodes2sequences = ReadCodesToSequences(filters='', lower=False, num_words=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class Padder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, max_len=TIMESTEPS):\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = pad_sequences(X, maxlen=self.max_len)\n",
    "        return X\n",
    "    \n",
    "padder = Padder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, LSTM, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=OUTPUT_SIZE, input_length=TIMESTEPS, batch_input_shape=(BATCH_SIZE, TIMESTEPS)))\n",
    "    model.add(LSTM(units=OUTPUT_SIZE, batch_input_shape=(BATCH_SIZE, TIMESTEPS, OUTPUT_SIZE), stateful=True, return_sequences=False))\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "    model.compile(**COMPILE_PARAMS)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "sklearn_model = KerasClassifier(\n",
    "    build_fn=create_model, \n",
    "    epochs=1, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(readcodes2sequences, padder, sklearn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_preds = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Accuracy: {:.2f} %'.format(100 * accuracy_score(y_preds, y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 52\n",
    "readcodes = X_test[idx]\n",
    "\n",
    "print('Flagging period {}'.format(idx))\n",
    "print('-' * 50)\n",
    "print(' '.join(readcodes.split()))\n",
    "print('-' * 50)\n",
    "print('Probability array: ', pipeline.predict_proba([readcodes]))\n",
    "print('Predicted class: ', lb.classes_[y_preds[idx]])\n",
    "print('Actual class: ', lb.classes_[y_true[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=lb.classes_.tolist())\n",
    "explanation = explainer.explain_instance(readcodes, pipeline.predict_proba, top_labels=3, distance_metric='cosine', bow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
