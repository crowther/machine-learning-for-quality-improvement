{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import datetime\n",
    "import pickle\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FLAGGING_DF_FILE = 'output/generate-flagging-periods.pkl'\n",
    "INPUT_RECORDS_FILE = 'input/danWhyStopFlaggingRecordsWithoutDuplicateGroupsOrCodes.txt'\n",
    "INPUT_DEMOGRAPHICS_FILE = 'input/danWhyStopFlaggingDemographics.txt'\n",
    "\n",
    "OUTPUT_TRAIN_DF_FILE = 'output/train-df.pkl'\n",
    "OUTPUT_TEST_DF_FILE = 'output/test-df.pkl'\n",
    "OUTPUT_TOK_FILE = 'output/tok.pkl'\n",
    "OUTPUT_X_TRAIN_FILE = 'output/encoded-X-train.pkl'\n",
    "OUTPUT_X_TEST_FILE = 'output/encoded-X-test.pkl'\n",
    "OUTPUT_Y_TRAIN_FILE = 'output/encoded-y-train.pkl'\n",
    "OUTPUT_Y_TEST_FILE = 'output/encoded-y-test.pkl'\n",
    "\n",
    "WINDOW = pd.DateOffset(months=1)\n",
    "TEST_SIZE = 0.2\n",
    "TIMESTEPS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_start_time = datetime.datetime.now()\n",
    "print('{} started at {}'.format(sys.argv[0], script_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading data...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "flagging_df = pd.read_pickle(INPUT_FLAGGING_DF_FILE)\n",
    "demographics_df = pd.read_csv(INPUT_DEMOGRAPHICS_FILE, index_col=['PatID'])\n",
    "demographics_df.Sex.replace(['F', 'M'], [0, 1], inplace=True)\n",
    "records_df = pd.read_csv(INPUT_RECORDS_FILE, index_col=['EntryDate'], parse_dates=['EntryDate'], encoding = \"ISO-8859-1\")\n",
    "records_df = records_df.loc[records_df.index > '2009-03-30', :]\n",
    "RECORDS_START_DATE, RECORDS_END_DATE = records_df.index.min(), records_df.index.max()\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relevant_df(all_flagging_df, all_records_df):\n",
    "    entry_dates = records_df.index.get_level_values('EntryDate')\n",
    "    relevant_dfs = []\n",
    "    for row in all_flagging_df.itertuples():\n",
    "        pt, start_date, stop_date, reason = row.Index[0], row.Index[1], row.Index[2], row.ReasonStoppedFlagging\n",
    "        start_age, stop_age = row.AgeAtFlagging, row.AgeAtStopFlagging\n",
    "        flag_count, year_started_flagging, year_stopped_flagging = row.FlagCount, row.YearStartedFlagging, row.YearStoppedFlagging\n",
    "        is_pt_and_relevant = ((pt == records_df.PatID) & (entry_dates > (start_date - WINDOW)) & (entry_dates < (stop_date + WINDOW)))\n",
    "        relevant_records_df = all_records_df.loc[is_pt_and_relevant, :]\n",
    "        if relevant_records_df.empty:\n",
    "            continue\n",
    "        readcode_text = generate_readcode_text(relevant_records_df)\n",
    "        relevant_dfs.append(pd.DataFrame(\n",
    "            data={'PatID': [pt], 'StartDate': [start_date], 'StopDate': [stop_date], 'Reason': [reason], 'ReadCodeText': [readcode_text]}, \n",
    "            columns=['PatID', 'StartDate', 'StopDate', 'Reason', 'ReadCodeText']\n",
    "        ))\n",
    "    return pd.concat(relevant_dfs, ignore_index=True)\n",
    "        \n",
    "def generate_readcode_text(relevant_records_df):\n",
    "    text = relevant_records_df.ReadCode.to_string(header=False, index=False).splitlines()\n",
    "    text = [t.strip() for t in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating relevant DataFrame...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "relevant_df = generate_relevant_df(flagging_df, records_df)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenising...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "relevant_texts = relevant_df.ReadCodeText\n",
    "tok = Tokenizer(filters='', lower=False)\n",
    "tok.fit_on_texts(relevant_texts)\n",
    "VOCAB_SIZE = len(tok.word_index) + 1\n",
    "relevant_sequences = tok.texts_to_sequences(relevant_texts)\n",
    "padded_sequences = pad_sequences(relevant_sequences, maxlen=TIMESTEPS)\n",
    "relevant_df['ReadCodeSequence'] = [x for x in padded_sequences]\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating training and testing sets...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "train_df, test_df, y_train, y_test = train_test_split(\n",
    "    relevant_df, \n",
    "    relevant_df.pop('Reason'), \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=1337\n",
    ")\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing data...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "X_train = np.array([np.array(x) for x in train_df.ReadCodeSequence])\n",
    "X_test = np.array([np.array(x) for x in test_df.ReadCodeSequence])\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(OUTPUT_TRAIN_DF_FILE)\n",
    "test_df.to_pickle(OUTPUT_TEST_DF_FILE)\n",
    "with open(OUTPUT_TOK_FILE, 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "X_train.dump(OUTPUT_X_TRAIN_FILE)\n",
    "X_test.dump(OUTPUT_X_TEST_FILE)\n",
    "y_train.to_pickle(OUTPUT_Y_TRAIN_FILE)\n",
    "y_test.to_pickle(OUTPUT_Y_TEST_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
